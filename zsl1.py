# -*- coding: utf-8 -*-
"""zsl1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CpNGyvKZjhPdzQFLRHbvg-R7tVplFXed
"""

#!pip uninstall transformers -y
# !pip install transformers

# from google.colab import files
# uploaded = files.upload()

import transformers
print(transformers.__version__)

# !ls -l

import pandas as pd
import io
from encoder import labelEncode
from compat import  getProbsSoftmax
from sklearn.metrics import accuracy_score

# !pip install tensorflow==2.5.0

import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel #TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
bert_model = TFAutoModel.from_pretrained(checkpoint)

train_data = pd.read_excel("train_data.xlsx",engine = "openpyxl")
print("Data loaded............................")
train_data = train_data.dropna()
train_data = train_data.sample(10000)

def return_embeddings(sequences):
  tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
  output = bert_model(**tokens)
  for d in output.items():
    last_hidden_state = d[1]
  return tf.reduce_mean(last_hidden_state, 1)

embeddings_items =  return_embeddings(train_data["item"].tolist())
embeddings_labels = return_embeddings(train_data["category"].tolist())

print("embedding label shape: ",tf.shape(embeddings_labels))

embedding_dim = (tf.shape(embeddings_items)[1]).numpy()

print("embedding items shape: ", embeddings_items)

test_data = pd.read_excel("test_data_with_zsl_labels.xlsx",engine = "openpyxl")
print("Data loaded............................")
test_data = test_data.dropna()
test_data = test_data.sample(500)

print(test_data)

test_embeddings_items =  return_embeddings(test_data["item"].tolist())
test_embeddings_labels = return_embeddings(test_data["category"].tolist())

'''
We keep 3 custom layers: one simply outputs W which is a trainable parameter, the second takes W from first and Phi_X and outputs Phi_X*W, 
the third takes Phi_X*W from the second and W from the first and computes the loss function. The model is comprised of these three layers.
'''
# class FirstLinear(tf.keras.layers.Layer):
#     def __init__(self, input_dim):
#         super(FirstLinear, self).__init__()
#         self.W = self.add_weight(
#             shape=(input_dim, input_dim), initializer="random_normal", trainable=True
#         )

#     def call(self, dummy):
#         return self.W

# class SecondLinear(tf.keras.layers.Layer):
#     def __init__(self):
#         super(SecondLinear, self).__init__()

#     def call(self, input_W, input_Phi_X):
#         return tf.matmul(input_Phi_X, input_W)

# class ActivityRegularizationLayer(tf.keras.layers.Layer):
#     def __init__(self, lambda_reg=1e-2):
#         super(ActivityRegularizationLayer, self).__init__()
#         self.lambda_reg = lambda_reg
#         self.I = tf.eye(embedding_dim)

#     def call(self, input_Phi_X_W, input_W):
#         loss = tf.math.square(tf.norm(input_Phi_X_W - embeddings_labels)) + tf.math.scalar_mul(self.lambda_reg, tf.math.square(tf.norm(input_W - self.I)))
#         self.add_loss(loss)
 
# class SparseMLP(tf.keras.layers.Layer):
#   """Stack of Linear layers with our custom regularization loss."""

#   def __init__(self, embedding_dim):
#       super(SparseMLP, self).__init__()
#       self.firstLinear = FirstLinear(embedding_dim)
#       self.secondLinear = SecondLinear()
#       self.regularization = ActivityRegularizationLayer()

#   def call(self, Phi_X):
#       W = self.firstLinear(1)
#       Phi_X_W = self.secondLinear(W, Phi_X)
#       return self.regularization(Phi_X_W, W)

# mlp = SparseMLP(embedding_dim)

# inputs = tf.keras.Input(shape=(embedding_dim,))
# outputs = mlp(inputs)
# model = tf.keras.Model(inputs, outputs)

# # tf.keras.utils.plot_model(model, show_shapes=True)

# model.compile(optimizer="adam")

def squared_loss(y_true,y_pred):  
    #print(tf.shape(y_true))
    #print(tf.shape(y_pred))
    squared_diff = tf.square(y_true-y_pred)
    #print(tf.shape(squared_diff))
    return tf.reduce_mean(squared_diff,axis = 1)


class EyeRegularizer(tf.keras.regularizers.Regularizer):

    def __init__(self, strength,units):
        self.strength = strength
        self.eye = tf.eye(units)

    def __call__(self, x):
        return self.strength * tf.reduce_sum(tf.square((x-self.eye)))


items = tf.keras.Input(shape = 768)
proj = tf.keras.layers.Dense(units = 768,activation = "linear", 
                             kernel_regularizer = EyeRegularizer(strength=1e-3,units = 768))(items)

model= tf.keras.Model(inputs = items, outputs = proj)

print(model.summary())

model.compile(optimizer = "adam", loss = squared_loss, run_eagerly = True)

encoder,train_data = labelEncode(train_data,le = None)
test_data = labelEncode(test_data,le = encoder)

classes = encoder.classes_.tolist()

class_embeddings = return_embeddings(classes)

SKIP_TRAIN = False
if not SKIP_TRAIN:

    for i in range(10):

        history = model.fit(embeddings_items, embeddings_labels, batch_size = 64,epochs = 2,
                            validation_data = (test_embeddings_items,test_embeddings_labels))
        proj = model.predict(test_embeddings_items)
        y_pred = getProbsSoftmax(proj,class_embeddings)
        y_true = test_data['category'].tolist()
        print("accuracy: ", accuracy_score(y_true,y_pred))
    

SKIP_TEST = False




